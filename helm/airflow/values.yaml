nameOverride: ""
fullnameOverride: ""

imagePullPolicy: IfNotPresent
imagePullSecrets: []
# - name: regcred

# The executor class that airflow should use. Choices include
# SequentialExecutor, LocalExecutor, CeleryExecutor, DaskExecutor, KubernetesExecutor
executor: CeleryExecutor

# Airflow uses Fernet to encrypt passwords in the connection configuration.
# Generate fernetKey with:
# ```python
# from cryptography.fernet import Fernet
# fernet_key= Fernet.generate_key()
# print(fernet_key.decode())
# ```
# More info at:
# - https://airflow.apache.org/docs/stable/security.html#securing-connections
fernetKey: ""

airflow:
  core:
    load_examples: false
    load_default_connections: false
  webserver:
    expose_config: true

# Airflow FAB-based webserver configs
# example: https://github.com/apache/airflow/blob/master/airflow/config_templates/default_webserver_config.py
webserver_config: ""

# examples:
# * https://github.com/astronomer/airflow-chart/blob/master/templates/configmap.yaml#L124
# * https://github.com/apache/airflow/blob/master/airflow/config_templates/airflow_local_settings.py
airflow_local_settings: ""
  # def pod_mutation_hook(pod: Pod):
  #   pod.annotations['airflow.apache.org/launched-by'] = 'Tests'

# Commons k8s configurations. It can be overridden by
# airflow components: webserver, scheduler, worker, flower
commons:
  image:
    repository: hub.teko.vn/dataops/airflow
    tag: 1.10.12
  replicas: null
  command: []
  args: []
  env: []

  labels: {}
  podLabels: {}
  annotations: {}
  podAnnotations: {}

  resources: {}

  schedulerName: null
  runtimeClassName: null
  priorityClassName: null

  affinity: {}
  tolerations: []
  nodeSelector: {}

  extraVolumes: []
  extraVolumeMounts: []

webserver:
  args: [webserver]

scheduler:
  args: [scheduler]

worker:
  args: [worker]
  terminationPeriod: 120  # 2 minutes to worker complete its tasks

flower:
  args: [flower]
  service:
    type: ClusterIP
    port: 5555
    nodePort:
    annotations: {}
  ingress:
    enabled: false
    annotations: {}
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
    hosts:
      - host: airflow.local
        path: /flower
    tls: []
    #  - secretName: airflow.local-tls
    #    hosts:
    #      - airflow.local

dags:
  path: /opt/airflow/dags
  doNotPickle: false
  # DAGs fetcher, options:
  # * none: DAGs are embedded inside the Docker image
  # * git: sync DAGs from git repo
  # * volume: using a share file system like S3FS,...
  fetcher: git

  # Beaware when use git fetcher, the real `dags_folder`
  # will changing to `{{.dags.path}}/repo/{{.dags.git.subPath}}`
  git:
    repo:    ""
    branch:  master
    subPath: ""

    image:
      repository: k8s.gcr.io/git-sync
      tag:   v3.1.5

    auth:
      username:
      password:

      sshKey:
      externalSshKeySecret:
        name:
        key:

    syncInterval: 60

  volume:
    subPath: ""

    # Option 1: helm managed PVC
    storageClass: null
    accessMode: ReadWriteMany
    size: 1G

    # Option 2: existing PVC
    existingClaim: null

logs:
  # Logging location configuration.
  # For remote logging:
  #   * Amazon S3:          s3://bucket/path/to/logs
  #   * Amazon CloudWatch:  cloudwatch://...
  #   * Google GCS:         gs://bucket/path/to/logs
  #   * Google StackDriver: stackdriver://...
  #   * Azure WASB:         wasbs://...
  #   * ElasticSearch:      es://host:port
  # For local logging:
  #   * /path/to/logs
  #   * files://path/to/logs
  path: /opt/airflow/logs

  # Airflow Connection ID for remote logging
  remoteConnId:

  # Airflow job log level
  level: INFO

  # Storage configuration for local logs
  persistence:
    enabled: false

    subPath: ""

    # Option 1: helm managed PVC
    storageClass: null
    accessMode: ReadWriteMany
    size: 1G

    # Option 2: existing PVC
    existingClaim: null

postgresql:
  enabled: true

  postgresqlUsername: postgres
  postgresqlPassword: airflow
  postgresqlDatabase: airflow

  replication:
    enabled: false

  persistence:
    enabled: true

# See: https://docs.sqlalchemy.org/en/13/dialects/index.html
externalDatabase: ""

redis:
  enabled: true

  password: airflow

  cluster:
    enabled: false

  sentinel:
    enabled: false

  master:
    persistence:
      enabled: true

# See: https://docs.celeryproject.org/en/stable/getting-started/brokers/index.html
externalCeleryBroker: ""

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name:

rbac:
  # If true, create & use RBAC resources
  create: true
  # If true, create and use PodSecurityPolicy
  pspEnable: true
  # The name of the ServiceAccount to use.
  # If not set and create is true, a name is generated using the fullname template
  name:
  # enable spark-on-k8s-operator
  sparkOperator:
    enable: true
    namespace: ""

podDisruptionBudget:
  enabled: false
  # minAvailable: 1
  # maxUnavailable:

service:
  type: ClusterIP
  port: 8080
  nodePort:
  annotations: {}
  sessionAffinity: "None"
  sessionAffinityConfig: {}

ingress:
  enabled: false
  annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
  hosts:
    - host: airflow.local
      path: /
  tls: []
  #  - secretName: airflow.local-tls
  #    hosts:
  #      - airflow.local

## Prometheus Exporter / Metrics
## ref: https://epoch8.github.io/2018/08/03/monitoring-airflow-with-prometheus.html
metrics:
  enabled: false

  ## Prometheus Service Monitor
  ## ref: https://github.com/coreos/prometheus-operator
  ##      https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint
  serviceMonitor:
    enabled: false
    namespace:
    labels: {}

    ## Additional serviceMonitor configs:
    # interval: 30s
    # scrapeTimeout: 10s
    # honorLabels: false
    # metricRelabelings: []
