apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-conf
data:
  spark-defaults.conf.template: |
    spark.driver.cores          1
    spark.driver.memory         2G
    spark.executor.cores        1
    spark.executor.memory       2G
    spark.executor.instances    2

    # spark.master              yarn
    # spark.master              spark://spark.kites.rocks:7077
    spark.master              k8s://https://kubernetes.default.svc:443
    spark.scheduler.mode      FAIR

    spark.driver.host                             ${KUBERNETES_POD_IP}
    spark.kubernetes.namespace                    ${KUBERNETES_NAMESPACE}
    spark.kubernetes.driver.pod.name              ${KUBERNETES_POD_NAME}
    spark.kubernetes.container.image              hub.teko.vn/data/spark:2.4.4-hadoop-3.1.3-python-3.7
    spark.kubernetes.container.image.pullPolicy   Always

    spark.kubernetes.driver.label.hub.jupyter.org/username        ${JUPYTERHUB_USER}
    spark.kubernetes.executor.label.hub.jupyter.org/username      ${JUPYTERHUB_USER}
    spark.kubernetes.driver.annotation.hub.jupyter.org/username   ${JUPYTERHUB_USER}
    spark.kubernetes.executor.annotation.hub.jupyter.org/username ${JUPYTERHUB_USER}

    # spark.eventLog.enabled         true
    # spark.eventLog.dir             hdfs://hadoop.kites.rocks:8020/apps/spark/spark-events
    # spark.eventLog.compress        true
    # spark.history.ui.port          18080
    # spark.history.fs.logDirectory  hdfs://hadoop.kites.rocks:8020/apps/spark/spark-events

    # spark.shuffle.service.enabled  true
    # spark.shuffle.service.port     7337

  spark-env.sh.template: |
    #!/usr/bin/env bash

    JAVA_HOME="$(readlink -f /usr/bin/java | sed "s:/bin/java::")"

    HADOOP_CONF_DIR="${HADOOP_CONF_DIR:-$HADOOP_HOME/etc/hadoop}"
    YARN_CONF_DIR="${YARN_CONF_DIR:-$HADOOP_HOME/etc/hadoop}"
    HDFS_CONF_DIR="${HDFS_CONF_DIR:-$HADOOP_HOME/etc/hadoop}"

    SPARK_DIST_CLASSPATH="$(${HADOOP_HOME}/bin/hadoop classpath)"
    if [ ! -z "${SPARK_EXTRA_CLASSPATH}" ]; then
        SPARK_DIST_CLASSPATH="${SPARK_DIST_CLASSPATH}:${SPARK_EXTRA_CLASSPATH}"
    fi

    SPARK_IDENT_STRING="${JUPYTERHUB_USER}"
    HADOOP_USER_NAME="${JUPYTERHUB_USER}"

  spark-bootstrap-hook.sh: |
    #!/usr/bin/env bash

    if ! command -v envsubst &>/dev/null; then
      apt-get update
      apt-get install -y gettext-base
    fi

    HERE=$(dirname "${BASH_SOURCE[0]}")
    SPARK_CONF_DIR="${SPARK_CONF_DIR:-$SPARK_HOME/conf}"

    envsubst < "${HERE}/spark-defaults.conf.template" > "${SPARK_CONF_DIR}/spark-defaults.conf"
    cp         "${HERE}/spark-env.sh.template"          "${SPARK_CONF_DIR}/spark-env.sh"
